{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db9619f-7cc4-48c9-8854-de179876e0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: dlopen(/Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch_scatter/_version_cpu.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <E57B6A01-82F8-3C7E-AE6D-AE7FA09C6614> /Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch_scatter/_version_cpu.so\n",
      "  Expected in:     <AEDB2D9B-AE02-3964-90EC-49E2AD5A10A1> /Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: dlopen(/Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch_cluster/_version_cpu.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <BF13A8D9-C637-3AAC-BA9B-800642AA6D9D> /Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch_cluster/_version_cpu.so\n",
      "  Expected in:     <AEDB2D9B-AE02-3964-90EC-49E2AD5A10A1> /Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: dlopen(/Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch_spline_conv/_version_cpu.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <295D5E1A-8A25-3C41-86C8-66ACA79CC8B9> /Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch_spline_conv/_version_cpu.so\n",
      "  Expected in:     <AEDB2D9B-AE02-3964-90EC-49E2AD5A10A1> /Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warnings.warn(\n",
      "/Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: dlopen(/Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch_sparse/_version_cpu.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <F0B43E83-2685-37E2-913C-9F61B8C68F9B> /Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch_sparse/_version_cpu.so\n",
      "  Expected in:     <AEDB2D9B-AE02-3964-90EC-49E2AD5A10A1> /Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "# importing relevant libraries and packages\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GraphConv, global_mean_pool\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import snntorch as snn\n",
    "\n",
    "# category mappings for LDL as integers\n",
    "ldl_mapping = {\n",
    "    'Very Low Detection (High Sensitivity)': 0,\n",
    "    'Low Detection': 1,\n",
    "    'Moderate Detection': 2,\n",
    "    'High Detection (Lower Sensitivity)': 3,\n",
    "    'Very High Detection (Low Sensitivity)': 4\n",
    "}\n",
    "\n",
    "def extract_features(descriptors):\n",
    "    numerical_values = []\n",
    "    for key, value in descriptors.items():\n",
    "        if key == \"CID\":\n",
    "            continue\n",
    "        if isinstance(value, (int, float)):\n",
    "            numerical_values.append(float(value))\n",
    "        elif isinstance(value, str) and value.replace('.', '', 1).isdigit():\n",
    "            numerical_values.append(float(value))\n",
    "    return torch.tensor(numerical_values, dtype=torch.float)\n",
    "\n",
    "# step 1: Find maximum feature length across JSON files\n",
    "def find_max_feature_length(json_files, folder_path):\n",
    "    max_feature_length = 0\n",
    "    for f in json_files:\n",
    "        data = json.load(open(os.path.join(folder_path, f), 'r'))\n",
    "        for section in ['detect_target', 'probe_material', 'test_medium_electrolyte']:\n",
    "            for item in data.get(section, []):\n",
    "                feature_length = len(extract_features(item['substance_descriptors']))\n",
    "                max_feature_length = max(max_feature_length, feature_length)\n",
    "    return max_feature_length\n",
    "\n",
    "# updated helper function to extract features and include Magpie Descriptors for \"inorganic solid\"\n",
    "def extract_features_with_magpie(descriptors, substance_type):\n",
    "    numerical_values = []\n",
    "    for key, value in descriptors.items():\n",
    "        if key == \"CID\" or (substance_type == \"inorganic solid\" and \"MagpieData\" in key):\n",
    "            continue\n",
    "        if isinstance(value, (int, float)):\n",
    "            numerical_values.append(float(value))\n",
    "        elif isinstance(value, str) and value.replace('.', '', 1).isdigit():\n",
    "            numerical_values.append(float(value))\n",
    "    \n",
    "    # add Magpie Descriptors for inorganic solids if they exist\n",
    "    if substance_type == \"inorganic solid\" and descriptors.get(\"Magpie Descriptors\") is not None:\n",
    "        magpie_descriptors = [\n",
    "            float(v) for k, v in descriptors[\"Magpie Descriptors\"].items() if isinstance(v, (int, float))\n",
    "        ]\n",
    "        numerical_values.extend(magpie_descriptors)\n",
    "    \n",
    "    return torch.tensor(numerical_values, dtype=torch.float)\n",
    "\n",
    "\n",
    "# updated aggregation function to use the new extract_features_with_magpie function\n",
    "def aggregate_by_type(items, max_feature_length):\n",
    "    types = {'small molecule': [], 'inorganic solid': [], 'polymer': []}\n",
    "    for item in items:\n",
    "        substance_type = item.get('substance_type', '').lower()\n",
    "        if substance_type in types:\n",
    "            # extract features, considering Magpie Descriptors if the type is \"inorganic solid\"\n",
    "            feature_tensor = F.pad(\n",
    "                extract_features_with_magpie(item['substance_descriptors'], substance_type),\n",
    "                (0, max_feature_length - len(extract_features_with_magpie(item['substance_descriptors'], substance_type)))\n",
    "            )\n",
    "            types[substance_type].append(feature_tensor)\n",
    "    \n",
    "    aggregated_features = []\n",
    "    for type_key in types:\n",
    "        if types[type_key]:\n",
    "            aggregated_features.append(torch.mean(torch.stack(types[type_key]), dim=0))\n",
    "        else:\n",
    "            aggregated_features.append(torch.zeros(max_feature_length))\n",
    "    \n",
    "    return torch.cat(aggregated_features)\n",
    "\n",
    "# generate graph from JSON data \n",
    "def generate_graph_from_json(data, max_feature_length):\n",
    "    nodes = []\n",
    "    edges = []\n",
    "\n",
    "    # aggregate target, probe, medium nodes with updated aggregation function\n",
    "    target_node = aggregate_by_type(data.get('detect_target', []), max_feature_length)\n",
    "    probe_node = aggregate_by_type(data.get('probe_material', []), max_feature_length)\n",
    "    medium_node = aggregate_by_type(data.get('test_medium_electrolyte', []), max_feature_length)\n",
    "\n",
    "    # conditions node\n",
    "    conditions_features = [\n",
    "        data.get(\"test_operating_temperature_celsius\", 0.0),\n",
    "        data.get(\"min_pH_when_testing\", -1.0),\n",
    "        data.get(\"max_pH_when_testing\", 0.0)\n",
    "    ]\n",
    "    conditions_node = F.pad(torch.tensor(conditions_features, dtype=torch.float),\n",
    "                            (0, max_feature_length * 3 - len(conditions_features)))\n",
    "\n",
    "    nodes.extend([target_node, probe_node, medium_node, conditions_node])\n",
    "\n",
    "    edges = [\n",
    "        (0, 1),  # Target -> Probe\n",
    "        (0, 2),  # Target -> Medium\n",
    "        (1, 2),  # Probe -> Medium\n",
    "        (3, 2)   # Conditions -> Medium\n",
    "    ]\n",
    "\n",
    "    x = torch.stack(nodes)\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    ldl_label = ldl_mapping.get(data.get(\"LDL_category\", \"\"), 0)\n",
    "    return Data(x=x, edge_index=edge_index, y=torch.tensor(ldl_label, dtype=torch.long))\n",
    "\n",
    "# load and process data--this time, getting only the original data\n",
    "folder_path = 'your_path'\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith('.json') and not f.endswith('_original.json')]\n",
    "max_feature_length = find_max_feature_length(json_files, folder_path)\n",
    "\n",
    "# the rest of the data loading and processing code remains unchanged\n",
    "def extract_and_concatenate_vectors(substances):\n",
    "    vector_data = []\n",
    "    for substance in substances:\n",
    "        descriptors = substance.get('substance_descriptors', {})\n",
    "        for key in [\"Morgan_128\", \"maccs_fp\", \"morgan_fp_128\"]:\n",
    "            value = descriptors.get(key, [])\n",
    "            if isinstance(value, list):\n",
    "                vector_data.extend(value)\n",
    "    return np.array(vector_data)\n",
    "\n",
    "snn_data = []\n",
    "gnn_data = []\n",
    "labels = []\n",
    "\n",
    "# find the maximum vector length across all files\n",
    "max_vector_length = 0\n",
    "all_vectors = []\n",
    "\n",
    "for f in json_files:\n",
    "    # load JSON data\n",
    "    data = json.load(open(os.path.join(folder_path, f), 'r'))\n",
    "\n",
    "    # generate graph and append to gnn_data\n",
    "    graph = generate_graph_from_json(data, max_feature_length)\n",
    "    if graph is not None:\n",
    "        gnn_data.append(graph)\n",
    "\n",
    "        # generate the spiking vector by concatenating target, probe, and medium\n",
    "        spiking_vector = np.concatenate([\n",
    "            extract_and_concatenate_vectors(data.get('detect_target', [])),\n",
    "            extract_and_concatenate_vectors(data.get('probe_material', [])),\n",
    "            extract_and_concatenate_vectors(data.get('test_medium_electrolyte', []))\n",
    "        ])\n",
    "        all_vectors.append(spiking_vector)\n",
    "\n",
    "        # update the maximum vector length if needed\n",
    "        max_vector_length = max(max_vector_length, len(spiking_vector))\n",
    "        # append the label for classification\n",
    "        labels.append(graph.y.item())\n",
    "\n",
    "# pad all_vectors to the maximum vector length\n",
    "padded_vectors = [np.pad(vec, (0, max_vector_length - len(vec)), 'constant') for vec in all_vectors]\n",
    "\n",
    "# append each padded vector directly to snn_data\n",
    "snn_data.extend(padded_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d34e062-cdf5-469c-946f-b9a3fdf281da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_s/xvhfw9y94xb68fp1jjfnn0dh0000gn/T/ipykernel_9340/1424544744.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  snn_data = torch.FloatTensor(snn_data)\n",
      "/Users/rpf/anaconda3/envs/snn-gnn-env/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "snn_data = torch.FloatTensor(snn_data)\n",
    "labels = torch.LongTensor(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(snn_data, labels, test_size=0.2, random_state=42)\n",
    "train_data, test_data = train_test_split(gnn_data, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# spike Latency Encoding\n",
    "def spike_latency_encoding(vector, num_time_steps):\n",
    "    spike_train = torch.zeros((num_time_steps, len(vector)))\n",
    "    for idx, value in enumerate(vector):\n",
    "        if value > 0:  # non-zero entries encode spikes\n",
    "            time_step = idx % num_time_steps  # assign spike based on index\n",
    "            spike_train[time_step, idx] = 1\n",
    "    return spike_train\n",
    "\n",
    "# Rate Encoding\n",
    "def rate_encoding(vector, num_time_steps):\n",
    "    spike_train = torch.zeros((num_time_steps, len(vector)))\n",
    "    for idx, value in enumerate(vector):\n",
    "        if value > 0:  # non-zero entries fire spikes\n",
    "            spike_train[:, idx] = 1  # fire consistently across all timesteps\n",
    "    return spike_train\n",
    "    \n",
    "# parameters\n",
    "num_time_steps = 16  # define a fixed number of time steps\n",
    "\n",
    "# apply encodings to all samples in snn_data\n",
    "latency_encoded_data = [spike_latency_encoding(vector, num_time_steps) for vector in snn_data]\n",
    "rate_encoded_data = [rate_encoding(vector, num_time_steps) for vector in snn_data]\n",
    "\n",
    "# define GNN model with customizable number of layers\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes, num_layers):\n",
    "        super(GNN, self).__init__()\n",
    "        self.convs = nn.ModuleList([GraphConv(num_features if i == 0 else hidden_channels, hidden_channels) for i in range(num_layers)])\n",
    "        self.lin = nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d1031c4-4285-4321-91e5-4e9e3f6c8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNeuronLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, beta, threshold):\n",
    "        super(LIFNeuronLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, hidden_size)  # fully connected layer\n",
    "        self.beta = beta  # leaky factor\n",
    "        self.threshold = threshold  # spiking threshold\n",
    "        self.membrane_potential = None  # dynamically initialized\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        hidden_size = self.fc.out_features\n",
    "\n",
    "        # initialize membrane potential if not done or shape doesn't match\n",
    "        if self.membrane_potential is None or self.membrane_potential.shape != (batch_size, hidden_size):\n",
    "            self.membrane_potential = torch.zeros(batch_size, hidden_size, device=x.device)\n",
    "\n",
    "        outputs = []  # store spikes for each time step\n",
    "        for t in range(x.shape[1]):  # iterate over time steps\n",
    "            input_at_t = x[:, t, :]  # input at the current time step\n",
    "            fc_output = self.fc(input_at_t)  # weighted input\n",
    "\n",
    "            # update membrane potential with leaky integration\n",
    "            self.membrane_potential = self.beta * self.membrane_potential + fc_output\n",
    "\n",
    "            # generate spikes using surrogate gradient\n",
    "            spikes = self.spike_function(self.membrane_potential)\n",
    "            # reset membrane potential for neurons that spiked\n",
    "            self.membrane_potential = self.membrane_potential * (1 - spikes)\n",
    "\n",
    "            outputs.append(spikes)  # save spikes for the current time step\n",
    "\n",
    "        # aggregate spikes over time (mean)\n",
    "        aggregated_output = torch.stack(outputs, dim=1).mean(dim=1)  # shape: [batch_size, hidden_size]\n",
    "        return aggregated_output\n",
    "\n",
    "    @staticmethod\n",
    "    def spike_function(membrane_potential):\n",
    "        \"\"\"\n",
    "        Surrogate gradient for spiking function.\n",
    "        Approximates the gradient with a smooth curve.\n",
    "        \"\"\"\n",
    "        threshold = 1.0\n",
    "        return (membrane_potential >= threshold).float() + \\\n",
    "               torch.sigmoid(membrane_potential - threshold) * (1 - (membrane_potential >= threshold).float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b850b7-fcee-4517-b9fa-5497b4455e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, beta, threshold):\n",
    "        super(SNNClassifier, self).__init__()\n",
    "        self.lif_layer = LIFNeuronLayer(input_size, hidden_size, beta, threshold)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lif_output = self.lif_layer(x)  # process input with LIF layer\n",
    "        return self.fc_out(lif_output)  # classify using the final layer\n",
    "\n",
    "class SurrogateCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SurrogateCrossEntropyLoss, self).__init__()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        apply surrogate gradient mechanism during training only.\n",
    "        \"\"\"\n",
    "        loss = self.ce_loss(outputs, targets)\n",
    "\n",
    "        # only register hook if gradients are being tracked (training mode)\n",
    "        if outputs.requires_grad:\n",
    "            outputs.register_hook(lambda grad: grad * torch.sigmoid(outputs) * (1 - torch.sigmoid(outputs)))\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc55f968-efda-424e-bb03-d136d66bf4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, snn_input_size, snn_hidden_size, gnn_input_size, gnn_hidden_size, num_classes, snn_beta=0.9, threshold=1.0, gnn_layers=2):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        self.snn = SNNClassifier(snn_input_size, snn_hidden_size, num_classes, beta=snn_beta, threshold=threshold)\n",
    "        self.gnn = GNN(gnn_input_size, gnn_hidden_size, num_classes, num_layers=gnn_layers)\n",
    "        self.fusion_layer = nn.Linear(num_classes * 2, num_classes)\n",
    "\n",
    "    def forward(self, snn_input, gnn_data):\n",
    "        snn_output = self.snn(snn_input)\n",
    "        gnn_output = self.gnn(gnn_data)\n",
    "        combined_output = torch.cat((snn_output, gnn_output), dim=1)\n",
    "        final_output = self.fusion_layer(combined_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe823fd4-b9be-4d35-bace-8b52373efd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multimodal(snn_loader, gnn_loader, model, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (snn_batch, gnn_data) in zip(snn_loader, gnn_loader):\n",
    "        snn_input, snn_labels = snn_batch\n",
    "        snn_input, snn_labels = snn_input.to(device), snn_labels.to(device)\n",
    "        gnn_data = gnn_data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass through the multimodal model\n",
    "        output = model(snn_input, gnn_data)\n",
    "\n",
    "        # reset SNN membrane potential to avoid state retention\n",
    "        if hasattr(model.snn.lif_layer, 'membrane_potential'):\n",
    "            model.snn.lif_layer.membrane_potential = None\n",
    "\n",
    "        # calculate loss\n",
    "        loss = criterion(output, gnn_data.y)\n",
    "\n",
    "        # backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(snn_loader)\n",
    "\n",
    "def test_multimodal(snn_loader, gnn_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (snn_batch, gnn_data) in zip(snn_loader, gnn_loader):\n",
    "            snn_input, snn_labels = snn_batch\n",
    "            snn_input, snn_labels = snn_input.to(device), snn_labels.to(device)\n",
    "            gnn_data = gnn_data.to(device)\n",
    "\n",
    "            # forward pass through multimodal model\n",
    "            output = model(snn_input, gnn_data)\n",
    "\n",
    "            # compute loss\n",
    "            loss = criterion(output, gnn_data.y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # save predictions and true labels\n",
    "            pred = output.argmax(dim=1)\n",
    "            preds.extend(pred.cpu().numpy())  # convert to numpy array\n",
    "            labels.extend(gnn_data.y.cpu().numpy())  # convert to numpy array\n",
    "\n",
    "    # calculate metrics\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    avg_loss = total_loss / len(snn_loader)\n",
    "\n",
    "    return avg_loss, accuracy, f1, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5ebd86c-abe0-49b3-9ea7-ea6149b453a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_original_data(folder_path):\n",
    "    json_files_org = [file for file in os.listdir(folder_path) if file.endswith('_original.json')]\n",
    "    max_feature_length_org = find_max_feature_length(json_files_org, folder_path)\n",
    "\n",
    "    snn_data = []\n",
    "    gnn_data = []\n",
    "    labels = []\n",
    "    \n",
    "    # find the maximum vector length across all files\n",
    "    max_vector_length = 0\n",
    "    all_vectors = []\n",
    "    \n",
    "    for f in json_files_org:\n",
    "        file_path = os.path.join(folder_path, f)  # construct the full path to the file\n",
    "        \n",
    "        # load JSON data\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "    \n",
    "        # generate graph and append to gnn_data\n",
    "        graph = generate_graph_from_json(data, max_feature_length_org)\n",
    "        if graph is not None:\n",
    "            gnn_data.append(graph)\n",
    "    \n",
    "            # generate the spiking vector by concatenating target, probe, and medium\n",
    "            spiking_vector = np.concatenate([\n",
    "                extract_and_concatenate_vectors(data.get('detect_target', [])),\n",
    "                extract_and_concatenate_vectors(data.get('probe_material', [])),\n",
    "                extract_and_concatenate_vectors(data.get('test_medium_electrolyte', []))\n",
    "            ])\n",
    "            all_vectors.append(spiking_vector)\n",
    "    \n",
    "            # update the maximum vector length if needed\n",
    "            max_vector_length = max(max_vector_length, len(spiking_vector))\n",
    "    \n",
    "            # append the label for classification\n",
    "            labels.append(graph.y.item())\n",
    "    \n",
    "    # pad all_vectors to the maximum vector length\n",
    "    padded_vectors = [np.pad(vec, (0, max_vector_length - len(vec)), 'constant') for vec in all_vectors]\n",
    "    \n",
    "    # append each padded vector directly to snn_data\n",
    "    snn_data.extend(padded_vectors)\n",
    "\n",
    "    snn_data_org = torch.FloatTensor(snn_data)\n",
    "    labels_org = torch.LongTensor(labels)\n",
    "    \n",
    "    # parameters\n",
    "    num_time_steps = 16  # define a fixed number of time steps\n",
    "    \n",
    "    # apply encodings to all samples in snn_data\n",
    "    latency_encoded_data = [spike_latency_encoding(vector, num_time_steps) for vector in snn_data_org]\n",
    "    #rate_encoded_data = [rate_encoding(vector, num_time_steps) for vector in snn_data_org]\n",
    "\n",
    "    encoded_data = torch.stack([spike_latency_encoding(vector, num_time_steps) for vector in snn_data_org])\n",
    "\n",
    "    #encoded_data = torch.stack([rate_encoding(vector, num_time_steps) for vector in snn_data_org])\n",
    "    \n",
    "    # define the batch size\n",
    "    batch_size = 64\n",
    "    dataset = TensorDataset(encoded_data, labels_org)\n",
    "    train_size = int(0.001 * len(encoded_data)) # evaluating in the entire original dataset\n",
    "    test_size = len(encoded_data) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    #snn_test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    _, X_test, _, y_test = train_test_split(snn_data_org, labels_org, test_size=0.999, random_state=42)\n",
    "    _, test_data = train_test_split(gnn_data, test_size=0.999, random_state=42)\n",
    "    \n",
    "    snn_test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = GeoDataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return snn_test_loader, test_loader, labels_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49388e47-76ff-4127-a81d-8cf9fa45e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gnn_input_size = gnn_data[0].x.shape[1]\n",
    "\n",
    "model = MultiModalModel(\n",
    "    snn_input_size=snn_data.shape[1],  # 1358 based on snn_data.shape\n",
    "    snn_hidden_size=64,\n",
    "    gnn_input_size=gnn_input_size,     # matching the actual feature size of gnn_data.x\n",
    "    gnn_hidden_size=64,\n",
    "    num_classes=5,\n",
    "    snn_beta=0.50,\n",
    "    threshold=1.0,\n",
    "    gnn_layers=7\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09153ba4-6500-4abe-9378-10ad9ee9d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "# prepare data for SNN--using latency encoding\n",
    "encoded_data = torch.stack([spike_latency_encoding(vector, num_time_steps) for vector in snn_data])\n",
    "\n",
    "# prepare data for SNN--using latency encoding\n",
    "#encoded_data = torch.stack([rate_encoding(vector, num_time_steps) for vector in snn_data])\n",
    "\n",
    "\n",
    "# define the batch size\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(encoded_data, labels)\n",
    "train_size = int(0.8 * len(encoded_data))\n",
    "test_size = len(encoded_data) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "snn_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "snn_test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f25f5191-cd7e-4f02-9073-4479e1c91b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Results - Test Set:\n",
      "Accuracy: 0.8889, F1: 0.8891, Precision: 0.8900, Recall: 0.8889\n",
      "Trial 1 Results - Original Dataset:\n",
      "Accuracy: 0.9068, F1: 0.9069, Precision: 0.9076, Recall: 0.9068\n",
      "\n",
      "Trial 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Results - Test Set:\n",
      "Accuracy: 0.8949, F1: 0.8954, Precision: 0.8986, Recall: 0.8949\n",
      "Trial 2 Results - Original Dataset:\n",
      "Accuracy: 0.9118, F1: 0.9122, Precision: 0.9148, Recall: 0.9118\n",
      "\n",
      "Trial 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Results - Test Set:\n",
      "Accuracy: 0.8857, F1: 0.8857, Precision: 0.8858, Recall: 0.8857\n",
      "Trial 3 Results - Original Dataset:\n",
      "Accuracy: 0.9076, F1: 0.9075, Precision: 0.9077, Recall: 0.9076\n",
      "\n",
      "Trial 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 Results - Test Set:\n",
      "Accuracy: 0.8803, F1: 0.8802, Precision: 0.8825, Recall: 0.8803\n",
      "Trial 4 Results - Original Dataset:\n",
      "Accuracy: 0.9043, F1: 0.9043, Precision: 0.9068, Recall: 0.9043\n",
      "\n",
      "Trial 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Results - Test Set:\n",
      "Accuracy: 0.8814, F1: 0.8815, Precision: 0.8833, Recall: 0.8814\n",
      "Trial 5 Results - Original Dataset:\n",
      "Accuracy: 0.9043, F1: 0.9044, Precision: 0.9056, Recall: 0.9043\n",
      "\n",
      "Trial 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 Results - Test Set:\n",
      "Accuracy: 0.8879, F1: 0.8876, Precision: 0.8907, Recall: 0.8879\n",
      "Trial 6 Results - Original Dataset:\n",
      "Accuracy: 0.9102, F1: 0.9102, Precision: 0.9126, Recall: 0.9102\n",
      "\n",
      "Trial 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 Results - Test Set:\n",
      "Accuracy: 0.8863, F1: 0.8864, Precision: 0.8887, Recall: 0.8863\n",
      "Trial 7 Results - Original Dataset:\n",
      "Accuracy: 0.9068, F1: 0.9071, Precision: 0.9083, Recall: 0.9068\n",
      "\n",
      "Trial 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 Results - Test Set:\n",
      "Accuracy: 0.8863, F1: 0.8867, Precision: 0.8886, Recall: 0.8863\n",
      "Trial 8 Results - Original Dataset:\n",
      "Accuracy: 0.9110, F1: 0.9113, Precision: 0.9126, Recall: 0.9110\n",
      "\n",
      "Trial 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 Results - Test Set:\n",
      "Accuracy: 0.8787, F1: 0.8790, Precision: 0.8805, Recall: 0.8787\n",
      "Trial 9 Results - Original Dataset:\n",
      "Accuracy: 0.9076, F1: 0.9080, Precision: 0.9089, Recall: 0.9076\n",
      "\n",
      "Trial 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Results - Test Set:\n",
      "Accuracy: 0.8868, F1: 0.8873, Precision: 0.8897, Recall: 0.8868\n",
      "Trial 10 Results - Original Dataset:\n",
      "Accuracy: 0.9093, F1: 0.9097, Precision: 0.9111, Recall: 0.9093\n",
      "\n",
      "Aggregated Results on Test Dataset:\n",
      "Accuracy - Mean: 0.8857, Std Dev: 0.0044\n",
      "F1 - Mean: 0.8859, Std Dev: 0.0045\n",
      "Precision - Mean: 0.8878, Std Dev: 0.0049\n",
      "Recall - Mean: 0.8857, Std Dev: 0.0044\n",
      "\n",
      "Aggregated Results on Original Dataset:\n",
      "Accuracy - Mean: 0.9080, Std Dev: 0.0025\n",
      "F1 - Mean: 0.9082, Std Dev: 0.0025\n",
      "Precision - Mean: 0.9096, Std Dev: 0.0029\n",
      "Recall - Mean: 0.9080, Std Dev: 0.0025\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "# number of trials and epochs per trial\n",
    "num_trials = 10\n",
    "num_epochs = 200\n",
    "\n",
    "folder_path_original = 'your_path'\n",
    "snn_test_loader_org, test_loader_org, labels_org = process_original_data(folder_path_original)\n",
    "\n",
    "# store results\n",
    "results = {\"accuracy\": [], \"f1\": [], \"precision\": [], \"recall\": []}\n",
    "original_results = {\"accuracy\": [], \"f1\": [], \"precision\": [], \"recall\": []}\n",
    "\n",
    "# save the initial model state\n",
    "initial_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# outer loop: Trials\n",
    "for trial in range(num_trials):\n",
    "    print(f\"\\nTrial {trial + 1}/{num_trials}\")\n",
    "    \n",
    "    # reset the model and optimizer\n",
    "    model.load_state_dict(initial_model_state)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # progress bar for epochs\n",
    "    with tqdm(total=num_epochs, desc=f\"Training Trial {trial + 1}\", leave=False) as pbar:\n",
    "        # Inner loop: Epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = train_multimodal(snn_train_loader, train_loader, model, optimizer, criterion, device)\n",
    "            pbar.update(1)  # update progress bar\n",
    "\n",
    "    # evaluate on test loaders\n",
    "    test_loss, accuracy, f1, precision, recall = test_multimodal(snn_test_loader, test_loader, model, criterion, device)\n",
    "    print(f\"Trial {trial + 1} Results - Test Set:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "\n",
    "    # store the metrics\n",
    "    results[\"accuracy\"].append(accuracy)\n",
    "    results[\"f1\"].append(f1)\n",
    "    results[\"precision\"].append(precision)\n",
    "    results[\"recall\"].append(recall)\n",
    "\n",
    "    # evaluate on the original dataset\n",
    "    test_loss_org, accuracy_org, f1_org, precision_org, recall_org = test_multimodal(\n",
    "        snn_test_loader_org, test_loader_org, model, criterion, device\n",
    "    )\n",
    "    print(f\"Trial {trial + 1} Results - Original Dataset:\")\n",
    "    print(f\"Accuracy: {accuracy_org:.4f}, F1: {f1_org:.4f}, Precision: {precision_org:.4f}, Recall: {recall_org:.4f}\")\n",
    "\n",
    "    # store the metrics for the original dataset\n",
    "    original_results[\"accuracy\"].append(accuracy_org)\n",
    "    original_results[\"f1\"].append(f1_org)\n",
    "    original_results[\"precision\"].append(precision_org)\n",
    "    original_results[\"recall\"].append(recall_org)\n",
    "\n",
    "# compute averages and standard deviations for the test dataset\n",
    "print(\"\\nAggregated Results on Test Dataset:\")\n",
    "for metric, values in results.items():\n",
    "    mean = np.mean(values)\n",
    "    std_dev = np.std(values)\n",
    "    print(f\"{metric.capitalize()} - Mean: {mean:.4f}, Std Dev: {std_dev:.4f}\")\n",
    "\n",
    "# compute averages and standard deviations for the original dataset\n",
    "print(\"\\nAggregated Results on Original Dataset:\")\n",
    "for metric, values in original_results.items():\n",
    "    mean = np.mean(values)\n",
    "    std_dev = np.std(values)\n",
    "    print(f\"{metric.capitalize()} - Mean: {mean:.4f}, Std Dev: {std_dev:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c725e8a-37a8-47cf-ae9c-a838f83640ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNN-GNN Env",
   "language": "python",
   "name": "snn-gnn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
